{"name":"Assignment 5: 3D Rasterization","tagline":"CMU 15-462/662 Assignment 5: 3D Rasterization","body":"# Overview\r\n\r\nIn this final assignment, you will extend some part of the code you developed in the first part of the class in order to make it richer, more interesting, more full-featured, and more fun.  Below we have provided a large number of suggestions, with a few specific starting points.  We suggest that you stay somewhere within the vicinity of these suggestions, since your existing code will make it (_reasonably_) easy to implement many of these algorithms.  However, if you're feeling really passionate about implementing one particular graphics algorithm, we're glad to hear about it!\r\n\r\nNote that we are not requiring you to hit specific technical bullet points—rather, your goal should be to build a more interesting and/or full-featured tool that extends a previous assignment in fun and creative ways.  You can achieve this goal in any way you like.  In other words, we are trying to prep you for building software in the real world, where there is no pre-baked script or list of bullet points you need in order to get \"points.\"  Also note that in many of these projects you will likely have to change some of the viewer/GUI in order to expose this new functionality to the user.  This type of coding can be tricky; if you are having trouble coming up with a good, simple way to modify the GUI, please consult with one of the TAs, who know the system well.  Also, note that code for some functionality (e.g., [Catmull-Clark subdivision](http://rosettacode.org/wiki/Catmull–Clark_subdivision_surface)) is widely available on the web.  Unlike previous assignments, in this project we are **encouraging you** to use existing code, in order to more rapidly build up a more full-featured/interesting/fun tool.  However, we will not give you credit simply for plugging in an existing source file or library!  We want to see that you are building a whole that is greater than the sum of its parts.  So please document and credit appropriately any use of external code or libraries when you hand in the assignment.\r\n\r\n# Administrative Details\r\n\r\nThe official due date for your project is **Friday, December 11 2015**.  However, we will accept submissions without penalty up until Wednesday, December 16 2015.  No further late days will be accepted after this final cutoff date.\r\n\r\nA proposal for your project must be submitted by **Wednesday, December 2nd 2015**.  In particular, the proposal should:\r\n\r\n* Be submitted to us as a web site with a publicly-accessible URL,\r\n* Specify your name (or possibly two names, if you work with a partner)\r\n* Give a brief description of the proposed project and what you hope to achieve—motivating images are extremely valuable here (e.g., \"in a perfect world, the output of our algorithm would look like this photograph...\")\r\n* Give a list of concrete steps you will take to implement the proposed algorithm.\r\n\r\nAs just one \"toy\" example, let's say your goal is to extend the _MeshEdit_ code to implement the most basic version of Catmull-Clark subdivision (which is _not_ enough for a real assignment!).  You might write:\r\n\r\n1. _\"We will first modify the viewer to load and display quadrilateral meshes.\"_\r\n2. _\"We will then implement simple linear subdivision by splitting each polygon at its center.\"_\r\n3. _\"Finally, we will write a routine that computes new vertex positions according to the actual Catmull-Clark routine.\"_\r\n\r\nFor a real project, these bullet points should be _slightly_ bigger and higher-level than the ones in the example above.  But they should still be split enough into small enough chunks that you have a concrete sense of what to do next.  If you're having trouble coming up with a concrete game plan, indicate which steps are giving you trouble and the staff will try to provide some guidance.  Overall, a big part of this project will be developing your skill in starting with a high-level conceptual target, and turning it into concrete, actionable items that you can actually achieve.  (This kind of ability is one key difference between a good developer and a stellar one!)\r\n\r\nFinally, to allow you to implement \"cooler stuff,\" **you may work with a partner if you so desire.**  However, partners must be clearly indicated on your proposal statement and final project submission.\r\n\r\n## Option E: 3D Rasterization\r\n\r\n![Example of order-independent transparency]( http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/3drasterization.png)\r\n\r\nYour first assignment focused primarily on 2D rasterization, but isn't so far from a full-blown 3D rasterization pipeline.  One possible extension for your final assignment is to implement a more full-featured 3D rasterization engine, in the spirit of OpenGL.  Additional features to consider might include:\r\n\r\n* implement the camera projection matrix\r\n* implement the viewing transform\r\n* perform perspective-correct attribute interpolation (color, texture coordinates, etc.)\r\n* implement [depth testing](https://en.wikipedia.org/wiki/Z-buffering) by adding a Z-buffer\r\n* implement [order-independent transparency](https://en.wikipedia.org/wiki/Order-independent_transparency) using an A-buffer\r\n* implement simple per-pixel lighting/shading\r\n* _advanced:_ implement shadow mapping\r\n* _advanced:_ add trilinear filtering (small change from your assignment 1 implementation to get the partial right)\r\n\r\nOverall, a fun question to think about is: what features can you implement in a software rasterizer that you _cannot_ (easily) implement in a hardware rasterizer.  Roughly speaking, modern graphics hardware is designed around a restricted set of rasterization features that were deemed to be important by the OpenGL standards board.  As a consequence, it can be tricky to do things like implement an A-buffer, which is not directly supported by the hardware.  But that doesn't mean OpenGL represents the \"one true way\" to design a rasterizer.  How does rasterization change in various scenarios?  Does virtual reality (VR) present new challenges for rasterization that don't show up in the traditional pipeline?\r\n\r\nAlso, what about geometric primitives other than triangles?  Graphics hardware is mainly a \"triangle machine,\" meaning that even when you draw a quad, a polygon, and even a point, it gets diced into triangles before being sent to the hardware.  The reason is chip area: if you can build one little piece of hardware that renders triangles, and render everything else in terms of triangles, then you can print a much smaller silicon wafer.  Software rasterizers are fun because they don't inherently have this restriction (i.e., you're not designing a chip, so you don't care about die area!).  So: what other primitives can you rasterize?  Interestingly enough, early NVIDIA chips supported direct rasterization of [bilinear patches](http://shaunramsey.com/research/bp/).  There are likewise algorithms for [visualizing higher-order Bézier patches](https://www.google.com/?gws_rd=ssl#q=ray+tracing+bezier+patches) or [subdivision surfaces](https://www.google.com/search?client=safari&rls=en&q=ray+tracing+subdivision+surfaces&ie=UTF-8&oe=UTF-8) using (you guessed it) ray tracing.  Much like high-performance algorithms for triangle rendering, the idea is to rasterize some region bounding (the projection of) a primitive like a Bézier patch, and then do a ray intersection with an implicit description of the patch to determine visibility, shading, etc.  The (_potential_) advantage from a systems point of view is that you don't have to dice patches into tiny little triangles, which means you don't have to waste as much bandwidth pushing all this data through the pipeline.\r\n\r\nLots of directions to consider here.  What's most exciting to you?\r\n\r\n## Option F: Advanced Monte Carlo Rendering\r\n\r\n![Advanced rendering example]( http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/advancedrendering.png)\r\n\r\nFinally, you could extend the physically-based renderer you started to develop for your third assignment—as you experienced first-hand, the basic path tracing algorithm does not converge very quickly!  It also doesn't capture some important (and beautiful!) physical phenomena.  Here are several specific suggestions for directions you might pursue:\r\n\r\n* _Improve performance of BVH construction._  For large and/or dynamic scenes, the cost of building a bounding volume hierarchy is specific.  Consider, for instance, running a large, mesh-based physical simulation of water, which you then want to ray trace in order to get beautiful caustics.  Lots of geometry changing in unpredictable ways—and _millions_ of rays to properly resolve the appearance of the surface.  You could improve the performance of your renderer by implementing one of the the top modern parallel BVH build algorithms such as\r\n   * [Efficient BVH Construction via Approximate Agglomerative Clustering](http://graphics.cs.cmu.edu/projects/aac/)\r\n   * [Fast Parallel Construction of High-Quality Bounding Volume Hierarchies](https://research.nvidia.com/publication/fast-parallel-construction-high-quality-bounding-volume-hierarchies)\r\n* _Improve performance of ray tracing._  There are also plenty of opportunities to improve the performance of the ray tracing itself.  One possibility is to implement fast packet-based ray tracing a la the method described in, [\"Ray Tracing Deformable Scenes using Dynamic Bounding Volume Hierarchies\"](http://graphics.stanford.edu/~boulos/papers/togbvh.pdf).\r\n* _Implement an advanced global sampling algorithm._ A very different way to reduce the cost of rendering an image is to improve the sampling strategy.  In other words, since each ray can be very expensive to intersect with the scene, it makes sense to put some effort into finding paths that are likely to carry a lot of \"light.\"  The ultimate goal is to \"beat the clock,\" i.e., your fancy strategy should not only give better estimates of the integral as a function of number of rays cast, but it should really decrease the overall render time for a fixed target level of noise (i.e., an estimate of the variance of your estimator).  We discussed a number of these methods in our _Advanced Rendering and Sampling_ lecture; a few you might consider are given in the following list (many of these methods are excellently described in the [PBRT book](http://www.pbrt.org)):\r\n   * [bidirectional path tracing](https://en.wikipedia.org/wiki/Path_tracing#Bidirectional_path_tracing)\r\n   * [Photon mapping](https://en.wikipedia.org/wiki/Photon_mapping)\r\n   * [Metropolis light transport](https://graphics.stanford.edu/papers/metro/)—this method is quite advanced, though the [\"hypercube\" picture](http://sirkan.iit.bme.hu/~szirmay/paper50_electronic.pdf) we discussed in class can make it a bit easier to implement.\r\n* _Improved sampling patterns_. We also discussed a variety of strategies for picking more intelligent sampling patterns (stratified sampling, low-discrepancy, blue noise, etc.).  You could implement a variety of strategies and, like the previous item, compare the estimated variance of the estimator relative to the wall clock time used to render.\r\n* _Irradiance caching_.  A method that is extremely important in practice (i.e., almost all \"real\" renderers use it) but that we didn't have a chance to discuss in class is _irradiance caching_.  In a nutshell the idea is to say: if illumination is varying slowly, you sometimes get away with interpolating existing samples rather than generating new ones.  Exactly _how_ you generate these samples (and how you interpolate them) is critical to getting good results, and plenty has been written on the subject.  But even a basic irradiance caching strategy could greatly improve the utility of your renderer, in the sense that you can get far smoother images with far fewer samples.\r\n\r\n## Option H: 2D Fluence Renderer\r\n\r\n![Diffuse 2D rendering]( https://github.com/Bryce-Summers/Bryce-Summers.github.io/blob/master/Images/Diffuse_Fluence_Rendering.png?raw=true )\r\n\r\nIn Assignment 3 we created images based on the amount of irradiance present on visible surfaces using ray tracing. What would happen if instead we asked how much irradiance passes a given point in space? This is the fluence of a given point and is understandable when visualized in 2D space. For this extension, you can start with either your Ray tracer or your assignment 1 rasterizer, but you will be performing operations on a 2D image buffer while also needing much of the mathematics and structures of your raytracer.\r\n\r\nTo start you off on the right track here are some great resources that you should look at:\r\n* [Secret Life of Photons]( https://benedikt-bitterli.me/tantalum/ )\r\n* [Zen Photon Garden] (http://zenphoton.com/)\r\n* [Transient Rendering] (http://www.cs.dartmouth.edu/~wjarosz/publications/jarabo14framework.html)\r\n\r\nHere are some concrete steps and features you could implement:\r\n* Light Sources : Start rays at light sources defined by a probability distribution of points, much like the light sources in p3. This will allow you to witness all 3 parts of a shadow: [Link](https://en.wikipedia.org/wiki/Umbra,_penumbra_and_antumbra).\r\n* 2D geometry : Compute the representations and intersections for 2D circles and line segments.\r\n* Primary Rays: Emit rays from light sources via antialiased line rasterization via Xiaolin Wu's line algorithm. Have the lines be drawn from the initial point on the light source in a randomly distributed direction until they hit a piece of geometry or the edge of the image. Have the lines plot their interpolated irradiance in a cache. You can then map the range of energy present in the cache to colors in the image. You may want to clamp the energy values at the light source, because otherwise the rest of the scene will always be too dark due to the extreme concentration of light at the light sources.\r\n* 2D Spatial Data Structures: implement some 2D spatial data structures such as axis aligned bounding boxes to accelerate the 2D geometry queries. Can you find optimizations for constructing these data structures in the limited confines of the 2D plane in the visible region that were not feasible in 3D?\r\n* 2D BRDF's and materials: Implement absorption, refraction, reflection brdf's for the pieces of geometry.\r\n* Implement Wavelength dependent refraction via the [Sellmeier Equation] (https://en.wikipedia.org/wiki/Sellmeier_equation) to achieve cool effects like in the Secret Life of Photons.\r\n* Render from an svg: Work on the SVG parser pipeline to specify scenes in svg / sml that may be rendered using your fluence renderer.\r\n* Correct the rasterization bias like in Secret Life of Photons.\r\n* Perform the proper gamma correction. Map different \r\n* [Transient Rendering] (http://www.cs.dartmouth.edu/~wjarosz/publications/jarabo14framework.html) : Create animations of light propagating throughout a scene. Also google Ramesh Raskar and check out his work with [Fempto Photography](https://www.ted.com/talks/ramesh_raskar_a_camera_that_takes_one_trillion_frames_per_second?language=en)","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}